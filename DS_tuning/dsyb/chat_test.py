from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# âœ… ä½ çš„æ¨¡å‹è·¯å¾„
merged_model_path = r"D:\PyCharm 2025.1.1.1\DS_tuning\merged_model"

print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")

# âœ… è‡ªåŠ¨è¯†åˆ« JSON æ ¼å¼åˆ†è¯å™¨ï¼ˆä¸è¦æ‰‹åŠ¨å…³ fastï¼‰
tokenizer = AutoTokenizer.from_pretrained(merged_model_path)

model = AutoModelForCausalLM.from_pretrained(
    merged_model_path,
    torch_dtype=torch.bfloat16,  # è‹¥ä½ çš„æ˜¾å¡æ”¯æŒ bfloat16 (RTX 40ç³»åˆ—æ”¯æŒ)
    device_map="auto"
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å¼€å§‹èŠå¤©æµ‹è¯•ã€‚\n")

while True:
    query = input("ğŸ§  ä½ ï¼š").strip()
    if query.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ ç»“æŸæµ‹è¯•ã€‚")
        break
    inputs = tokenizer(query, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("ğŸ¤– DSæ¨¡å‹ï¼š", response)
    print("-" * 60)
