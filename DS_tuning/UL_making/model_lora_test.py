import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# ==== é…ç½®éƒ¨åˆ† ====
BASE_MODEL = r"C:\Users\xunyi\Desktop\FlaskProject1\model\deepseek-ai\deepseek-llm-7b-chat"   # âœ… æ”¹æˆæœ¬åœ°è·¯å¾„ï¼ˆæ— å‘½åç©ºé—´ï¼‰
LORA_PATH = r"C:\Users\xunyi\Desktop\FlaskProject1\model\deepseek-lora-finetuned\checkpoint-57600"          # è®­ç»ƒå¥½çš„ LoRA æƒé‡è·¯å¾„
USE_4BIT = True                                   # æ˜¯å¦ä½¿ç”¨4bité‡åŒ–ï¼ˆçœæ˜¾å­˜ï¼‰

# ==== åŠ è½½ Tokenizer ====
print("ğŸ”¹ æ­£åœ¨åŠ è½½ Tokenizer ...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

# ==== åŠ è½½åŸºç¡€æ¨¡å‹ ====
print("ğŸ”¹ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ ...")
bnb_config = None
if USE_4BIT:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16 if USE_4BIT else torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

# ==== åˆå¹¶ LoRA æƒé‡ ====
print("ğŸ”¹ æ­£åœ¨åˆå¹¶ LoRA æƒé‡ ...")
model = PeftModel.from_pretrained(model, LORA_PATH)

# âš ï¸ å¦‚æœåªæ˜¯æƒ³åŠ è½½LoRAå¾®è°ƒï¼Œä¸å¿… merge
# model = model.merge_and_unload()

model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹èŠå¤©ï¼\n")

# ==== èŠå¤©å¾ªç¯ ====
def chat():
    # æç¤ºè¯å‰ç¼€ï¼šæ˜ç¡®è¦æ±‚ç»“åˆå¾®è°ƒçŸ¥è¯†å’Œé€šç”¨çŸ¥è¯†
    # æ›¿æ¢ä¸ºä½ çš„é¢†åŸŸï¼ˆå¦‚â€œå®‰å…¨ç›‘æ§å‘Šè­¦â€â€œè®¾å¤‡è¿ç»´â€ç­‰ï¼‰
    domain = "å®‰å…¨ç›‘æ§å‘Šè­¦"
    forbidden_phrases = ["ç³»ç»Ÿæ€§èƒ½åˆ†æ", "è§†è§‰æ£€æµ‹ä¸è¯­è¨€ç”Ÿæˆ", "å…¨é¢çš„å®‰å…¨ç›‘æŠ¤", "é«˜æ•ˆçš„å¼‚å¸¸å¤„ç†"]
    system_prompt = f"""
    ä½ æ˜¯{domain}é¢†åŸŸçš„ä¸“å®¶ï¼Œå›ç­”å¿…é¡»ä¸¥æ ¼éµå¾ªï¼š
    1. ç»å¯¹ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹å¥—è¯ï¼š{', '.join(forbidden_phrases)}
    2. å¿…é¡»å…ˆç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼ˆå¦‚â€œæ‘”å€’åæ€ä¹ˆåŠâ€æ˜¯è¦è‡ªæ•‘å»ºè®®ï¼‰ï¼Œå†é’ˆå¯¹æ€§å›ç­”ï¼›
    3. è‹¥é—®é¢˜æ¶‰åŠç”¨æˆ·è‡ªèº«è¡ŒåŠ¨ï¼ˆå¦‚æ‘”å€’ã€æ±‚åŠ©ï¼‰ï¼Œä¼˜å…ˆæä¾›å…·ä½“å¯æ“ä½œçš„æ­¥éª¤ï¼›
    4. è‹¥æ¶‰åŠç³»ç»Ÿï¼Œç®€è¦è¯´æ˜å³å¯ï¼Œé‡ç‚¹æ”¾åœ¨ç”¨æˆ·èƒ½ç†è§£çš„å†…å®¹ä¸Šã€‚
    """

    while True:
        user_input = input("ğŸ§‘â€ğŸ’¬ ä½ ï¼š").strip()
        if user_input.lower() in ["exit", "quit", "é€€å‡º", "q"]:
            print("ğŸ‘‹ å†è§ï¼")
            break

        # æ‹¼æ¥ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·è¾“å…¥ï¼Œå¼•å¯¼æ¨¡å‹å¹³è¡¡ä¸¤ç§çŸ¥è¯†
        prompt = f"{system_prompt}\nç”¨æˆ·é—®ï¼š{user_input}\nå›ç­”ï¼š"
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=True,  # ä¿ç•™ä¸€å®šéšæœºæ€§ï¼Œå…¼é¡¾ä¸¤ç§çŸ¥è¯†çš„çµæ´»æ€§
                temperature=0.6,  # ä¸­ç­‰æ¸©åº¦ï¼ˆ0.5-0.7ï¼‰ï¼Œå¹³è¡¡ç¡®å®šæ€§å’Œå¤šæ ·æ€§
                top_p=0.8,  # é€‚å½“æ‰©å¤§é‡‡æ ·èŒƒå›´ï¼Œå…è®¸é€šç”¨çŸ¥è¯†å‚ä¸
                repetition_penalty=1.1,  # è½»å¾®æƒ©ç½šé‡å¤ï¼Œé¿å…å•ä¸€çŸ¥è¯†å æ¯”è¿‡é«˜
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        # è§£æå¹¶æ¸…æ´—å›ç­”
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ç§»é™¤æç¤ºè¯å‰ç¼€ï¼Œåªä¿ç•™æ ¸å¿ƒå›ç­”
        if "å›ç­”ï¼š" in response:
            response = response.split("å›ç­”ï¼š")[-1].strip()
        # ç§»é™¤å¯èƒ½æ®‹ç•™çš„ç”¨æˆ·è¾“å…¥
        if response.startswith(user_input):
            response = response[len(user_input):].strip()

        print(f"ğŸ¤– æ¨¡å‹ï¼š{response}\n")


if __name__ == "__main__":
    chat()



