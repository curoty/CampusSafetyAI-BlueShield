import queue
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from queue import Queue

# ==== é…ç½®éƒ¨åˆ† ====
BASE_MODEL = r"model/deepseek-ai/deepseek-llm-7b-chat"
LORA_PATH = r"model/deepseek-lora-finetuned/checkpoint-57600"
USE_4BIT = True

model = None
tokenizer = None

# ==== åŠ è½½ Tokenizer ====
def run():
    """åŠ è½½DSæ¨¡å‹çš„çº¿ç¨‹å‡½æ•°"""
    global model, tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    # ==== åŠ è½½åŸºç¡€æ¨¡å‹ ====
    bnb_config = None
    if USE_4BIT:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        dtype=torch.bfloat16 if USE_4BIT else torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )

    # ==== åˆå¹¶ LoRA æƒé‡ ====
    print("ğŸ”¹ æ­£åœ¨åˆå¹¶ LoRA æƒé‡ ...")
    model = PeftModel.from_pretrained(model, LORA_PATH)


    model.eval()

def ask_ai_question(result_queue: Queue, *args):
    """é—®ç­”æ¥å£ï¼ˆéœ€åœ¨æ¨¡å‹åŠ è½½å®Œæˆåè°ƒç”¨ï¼‰"""
    global model, tokenizer
    if model is None or tokenizer is None:
        result_queue.put("é”™è¯¯ï¼šDSæ¨¡å‹æœªåŠ è½½å®Œæˆï¼Œè¯·ç¨åå†è¯•")
        return

    try:
        # ï¼ˆä¿æŒåŸæœ‰çš„ask_ai_questioné€»è¾‘ä¸å˜ï¼‰
        if len(args) not in (1, 2):
            result_queue.put("å‚æ•°é”™è¯¯ï¼šéœ€ä¼ å…¥1ä¸ªï¼ˆå†…å®¹ï¼‰æˆ–2ä¸ªï¼ˆæŒ‡ä»¤, å†…å®¹ï¼‰å‚æ•°")
            return

        if len(args) == 1:
            content = args[0]
            domain = "å®‰å…¨ç›‘æ§å‘Šè­¦"
            forbidden_phrases = ["ç³»ç»Ÿæ€§èƒ½åˆ†æ", "è§†è§‰æ£€æµ‹ä¸è¯­è¨€ç”Ÿæˆ", "å…¨é¢çš„å®‰å…¨ç›‘æŠ¤", "é«˜æ•ˆçš„å¼‚å¸¸å¤„ç†"]
            system_prompt = f"""
                ä½ æ˜¯{domain}é¢†åŸŸçš„ä¸“å®¶ï¼Œå›ç­”å¿…é¡»ä¸¥æ ¼éµå¾ªï¼š
                1. ç»å¯¹ç¦æ­¢ä½¿ç”¨ä»¥ä¸‹å¥—è¯ï¼š{', '.join(forbidden_phrases)}
                2. å¿…é¡»å…ˆç†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ï¼ˆå¦‚â€œæ‘”å€’åæ€ä¹ˆåŠâ€æ˜¯è¦è‡ªæ•‘å»ºè®®ï¼‰ï¼Œå†é’ˆå¯¹æ€§å›ç­”ï¼›
                3. è‹¥é—®é¢˜æ¶‰åŠç”¨æˆ·è‡ªèº«è¡ŒåŠ¨ï¼ˆå¦‚æ‘”å€’ã€æ±‚åŠ©ï¼‰ï¼Œä¼˜å…ˆæä¾›å…·ä½“å¯æ“ä½œçš„æ­¥éª¤ï¼›
                4. è‹¥æ¶‰åŠç³»ç»Ÿï¼Œç®€è¦è¯´æ˜å³å¯ï¼Œé‡ç‚¹æ”¾åœ¨ç”¨æˆ·èƒ½ç†è§£çš„å†…å®¹ä¸Šã€‚
                """
            prompt = f"{system_prompt}\nç”¨æˆ·é—®{content}\nå›ç­”ï¼š"

        else:
            instruction, content = args
            prompt = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{content}\nå›ç­”ï¼š"

        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "å›ç­”ï¼š" in response:
            response = response.split("å›ç­”ï¼š")[-1].strip()
        # ç§»é™¤å¯èƒ½æ®‹ç•™çš„ç”¨æˆ·è¾“å…¥
        if response.startswith(content):
            response = response[len(content):].strip()
        result_queue.put(response)
        print(response)

    except Exception as e:
        result_queue.put(f"å¤„ç†é”™è¯¯ï¼š{str(e)}")


def test():
    run()
    a = queue.Queue(maxsize=2)
    ask_ai_question(a, "ä½ æ˜¯æœ¬é¡¹ç›®çš„å‘Šè­¦åŠ©æ‰‹ï¼Œæ ¹æ®è¾“å…¥çš„é«˜å±ä¿¡æ¯ï¼Œè¿›è¡Œå‘Šè­¦å¹¶è¾“å‡ºåˆ°æœ€ç»ˆç»“æœåˆ°å‰ç«¯ç”¨æˆ·", "ç›‘æ§å‘Šè­¦è§¦å‘ï¼š00:01:41äºè€äººå±…ä½åŒºè¯†åˆ«åˆ°æ‘”å€’é«˜å±äº‹ä»¶ï¼Œå¸§æ¯”ä¾‹éªŒè¯é€šè¿‡")
